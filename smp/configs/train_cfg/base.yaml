# configs/train_cfg/base.yaml

# You can experiment with changing the optimizer or scheduler.
# Basically, it monitors avg_dice_score.
# Pay attention to the ealrystop patience argument. \
# It is related to the validation interval.

optimizer:
  name: Adam
  lr_backbone: 2e-5
  params:
    lr: 2e-4
    weight_decay: 1e-6

scheduler:
  name: CosineAnnealingWarmRestarts 
  params:
    T_0: 20            # epoch (1 cycle)
    T_mult: 2           # It decreases by 1/2 per cycle. (next cycle: T_0/2)
    eta_min: 0.000001   # 1e-6

trainer:
  max_epochs: 200
  check_val_every_n_epoch: 5    # validation interval
  accelerator: gpu
  devices: 1

callbacks:
  earlystopping:
    enabled: True
    params:
      monitor: avg_dice_score
      patience: 3               # validation interval * patience (15 patience)
      mode: max
  model_checkpoint:
    enabled: True
    params:
      monitor: avg_dice_score
      save_top_k: 3             # save top 3 weights in your logs dir (/checkpoints)
      mode: max

logger:
  tensorboard: True
  wandb: False                   # To use the wandb logger, you must init it first.
  project: arch                 # {unetplusplus}
  name: model_experiments    # {efficientnet-b7}_{rotate_aug_test}
