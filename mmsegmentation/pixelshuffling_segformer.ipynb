{"cells":[{"cell_type":"markdown","id":"63a83355","metadata":{"id":"63a83355"},"source":["# Import"]},{"cell_type":"code","execution_count":1,"id":"86e8c8e8","metadata":{"id":"86e8c8e8"},"outputs":[],"source":["import os\n","import json\n","from collections import OrderedDict, defaultdict"]},{"cell_type":"code","execution_count":2,"id":"f0a2d6c2","metadata":{"id":"f0a2d6c2"},"outputs":[{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n","  from .autonotebook import tqdm as notebook_tqdm\n"]}],"source":["import numpy as np\n","import pandas as pd\n","import cv2\n","from sklearn.model_selection import GroupKFold\n","import matplotlib.pyplot as plt\n","from prettytable import PrettyTable\n","from tqdm.auto import tqdm\n","\n","import torch\n","from torch import nn\n","import torch.nn.functional as F"]},{"cell_type":"code","execution_count":3,"id":"dbfce8fb","metadata":{"id":"dbfce8fb"},"outputs":[{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/albumentations/__init__.py:13: UserWarning: A new version of Albumentations is available: 1.4.21 (you have 1.4.18). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.\n","  check_for_updates()\n"]}],"source":["from mmseg.registry import DATASETS, TRANSFORMS, MODELS, METRICS\n","from mmseg.datasets import BaseSegDataset\n","from mmseg.models.segmentors import EncoderDecoder\n","from mmseg.models.decode_heads import ASPPHead, FCNHead, SegformerHead\n","from mmseg.models.utils.wrappers import resize\n","\n","\n","from mmengine.config import Config\n","from mmengine.dataset import Compose\n","from mmengine.runner import Runner, load_checkpoint\n","from mmengine.evaluator import BaseMetric\n","from mmengine.logging import MMLogger, print_log\n","from mmengine.structures import PixelData\n","\n","from mmcv.transforms import BaseTransform"]},{"cell_type":"code","execution_count":4,"id":"b9bdfd8a","metadata":{"id":"b9bdfd8a"},"outputs":[],"source":["# 데이터 경로를 입력하세요\n","\n","IMAGE_ROOT = \"train/DCM/\"\n","LABEL_ROOT = \"train/outputs_json/\""]},{"cell_type":"code","execution_count":5,"id":"c10ec100","metadata":{"id":"c10ec100"},"outputs":[],"source":["CLASSES = [\n","    'finger-1', 'finger-2', 'finger-3', 'finger-4', 'finger-5',\n","    'finger-6', 'finger-7', 'finger-8', 'finger-9', 'finger-10',\n","    'finger-11', 'finger-12', 'finger-13', 'finger-14', 'finger-15',\n","    'finger-16', 'finger-17', 'finger-18', 'finger-19', 'Trapezium',\n","    'Trapezoid', 'Capitate', 'Hamate', 'Scaphoid', 'Lunate',\n","    'Triquetrum', 'Pisiform', 'Radius', 'Ulna',\n","]"]},{"cell_type":"code","execution_count":6,"id":"808d4d86","metadata":{"id":"808d4d86"},"outputs":[],"source":["CLASS2IND = {v: i for i, v in enumerate(CLASSES)}"]},{"cell_type":"code","execution_count":7,"id":"14f20766","metadata":{"id":"14f20766"},"outputs":[],"source":["IND2CLASS = {v: k for k, v in CLASS2IND.items()}"]},{"cell_type":"code","execution_count":8,"id":"e41dcbce","metadata":{"id":"e41dcbce"},"outputs":[],"source":["pngs = {\n","    os.path.relpath(os.path.join(root, fname), start=IMAGE_ROOT)\n","    for root, _dirs, files in os.walk(IMAGE_ROOT)\n","    for fname in files\n","    if os.path.splitext(fname)[1].lower() == \".png\"\n","}\n","\n","jsons = {\n","    os.path.relpath(os.path.join(root, fname), start=LABEL_ROOT)\n","    for root, _dirs, files in os.walk(LABEL_ROOT)\n","    for fname in files\n","    if os.path.splitext(fname)[1].lower() == \".json\"\n","}"]},{"cell_type":"code","execution_count":9,"id":"8148c927","metadata":{"id":"8148c927"},"outputs":[],"source":["pngs = sorted(pngs)\n","jsons = sorted(jsons)"]},{"cell_type":"code","execution_count":17,"id":"8ce51812","metadata":{},"outputs":[],"source":["# Copyright (c) OpenMMLab. All rights reserved.\n","import math\n","import warnings\n","\n","import torch\n","import torch.nn as nn\n","import torch.utils.checkpoint as cp\n","from mmcv.cnn import Conv2d, build_activation_layer, build_norm_layer\n","from mmcv.cnn.bricks.drop import build_dropout\n","from mmcv.cnn.bricks.transformer import MultiheadAttention\n","from mmengine.model import BaseModule, ModuleList, Sequential\n","from mmengine.model.weight_init import (constant_init, normal_init,\n","                                        trunc_normal_init)\n","\n","from mmseg.registry import MODELS\n","from mmseg.models.utils import PatchEmbed, nchw_to_nlc, nlc_to_nchw\n","\n","\n","class MixFFN(BaseModule):\n","    \"\"\"An implementation of MixFFN of Segformer.\n","\n","    The differences between MixFFN & FFN:\n","        1. Use 1X1 Conv to replace Linear layer.\n","        2. Introduce 3X3 Conv to encode positional information.\n","    Args:\n","        embed_dims (int): The feature dimension. Same as\n","            `MultiheadAttention`. Defaults: 256.\n","        feedforward_channels (int): The hidden dimension of FFNs.\n","            Defaults: 1024.\n","        act_cfg (dict, optional): The activation config for FFNs.\n","            Default: dict(type='ReLU')\n","        ffn_drop (float, optional): Probability of an element to be\n","            zeroed in FFN. Default 0.0.\n","        dropout_layer (obj:`ConfigDict`): The dropout_layer used\n","            when adding the shortcut.\n","        init_cfg (obj:`mmcv.ConfigDict`): The Config for initialization.\n","            Default: None.\n","    \"\"\"\n","\n","    def __init__(self,\n","                 embed_dims,\n","                 feedforward_channels,\n","                 act_cfg=dict(type='GELU'),\n","                 ffn_drop=0.,\n","                 dropout_layer=None,\n","                 init_cfg=None):\n","        super().__init__(init_cfg)\n","\n","        self.embed_dims = embed_dims\n","        self.feedforward_channels = feedforward_channels\n","        self.act_cfg = act_cfg\n","        self.activate = build_activation_layer(act_cfg)\n","\n","        in_channels = embed_dims\n","        fc1 = Conv2d(\n","            in_channels=in_channels,\n","            out_channels=feedforward_channels,\n","            kernel_size=1,\n","            stride=1,\n","            bias=True)\n","        # 3x3 depth wise conv to provide positional encode information\n","        pe_conv = Conv2d(\n","            in_channels=feedforward_channels,\n","            out_channels=feedforward_channels,\n","            kernel_size=3,\n","            stride=1,\n","            padding=(3 - 1) // 2,\n","            bias=True,\n","            groups=feedforward_channels)\n","        fc2 = Conv2d(\n","            in_channels=feedforward_channels,\n","            out_channels=in_channels,\n","            kernel_size=1,\n","            stride=1,\n","            bias=True)\n","        drop = nn.Dropout(ffn_drop)\n","        layers = [fc1, pe_conv, self.activate, drop, fc2, drop]\n","        self.layers = Sequential(*layers)\n","        self.dropout_layer = build_dropout(\n","            dropout_layer) if dropout_layer else torch.nn.Identity()\n","\n","    def forward(self, x, hw_shape, identity=None):\n","        out = nlc_to_nchw(x, hw_shape)\n","        out = self.layers(out)\n","        out = nchw_to_nlc(out)\n","        if identity is None:\n","            identity = x\n","        return identity + self.dropout_layer(out)\n","\n","\n","class EfficientMultiheadAttention(MultiheadAttention):\n","    \"\"\"An implementation of Efficient Multi-head Attention of Segformer.\n","\n","    This module is modified from MultiheadAttention which is a module from\n","    mmcv.cnn.bricks.transformer.\n","    Args:\n","        embed_dims (int): The embedding dimension.\n","        num_heads (int): Parallel attention heads.\n","        attn_drop (float): A Dropout layer on attn_output_weights.\n","            Default: 0.0.\n","        proj_drop (float): A Dropout layer after `nn.MultiheadAttention`.\n","            Default: 0.0.\n","        dropout_layer (obj:`ConfigDict`): The dropout_layer used\n","            when adding the shortcut. Default: None.\n","        init_cfg (obj:`mmcv.ConfigDict`): The Config for initialization.\n","            Default: None.\n","        batch_first (bool): Key, Query and Value are shape of\n","            (batch, n, embed_dim)\n","            or (n, batch, embed_dim). Default: False.\n","        qkv_bias (bool): enable bias for qkv if True. Default True.\n","        norm_cfg (dict): Config dict for normalization layer.\n","            Default: dict(type='LN').\n","        sr_ratio (int): The ratio of spatial reduction of Efficient Multi-head\n","            Attention of Segformer. Default: 1.\n","    \"\"\"\n","\n","    def __init__(self,\n","                 embed_dims,\n","                 num_heads,\n","                 attn_drop=0.,\n","                 proj_drop=0.,\n","                 dropout_layer=None,\n","                 init_cfg=None,\n","                 batch_first=True,\n","                 qkv_bias=False,\n","                 norm_cfg=dict(type='LN'),\n","                 sr_ratio=1):\n","        super().__init__(\n","            embed_dims,\n","            num_heads,\n","            attn_drop,\n","            proj_drop,\n","            dropout_layer=dropout_layer,\n","            init_cfg=init_cfg,\n","            batch_first=batch_first,\n","            bias=qkv_bias)\n","\n","        self.sr_ratio = sr_ratio\n","        if sr_ratio > 1:\n","            self.sr = Conv2d(\n","                in_channels=embed_dims,\n","                out_channels=embed_dims,\n","                kernel_size=sr_ratio,\n","                stride=sr_ratio)\n","            # The ret[0] of build_norm_layer is norm name.\n","            self.norm = build_norm_layer(norm_cfg, embed_dims)[1]\n","\n","        # handle the BC-breaking from https://github.com/open-mmlab/mmcv/pull/1418 # noqa\n","        from mmseg import digit_version, mmcv_version\n","        if mmcv_version < digit_version('1.3.17'):\n","            warnings.warn('The legacy version of forward function in'\n","                          'EfficientMultiheadAttention is deprecated in'\n","                          'mmcv>=1.3.17 and will no longer support in the'\n","                          'future. Please upgrade your mmcv.')\n","            self.forward = self.legacy_forward\n","\n","    def forward(self, x, hw_shape, identity=None):\n","\n","        x_q = x\n","        if self.sr_ratio > 1:\n","            x_kv = nlc_to_nchw(x, hw_shape)\n","            x_kv = self.sr(x_kv)\n","            x_kv = nchw_to_nlc(x_kv)\n","            x_kv = self.norm(x_kv)\n","        else:\n","            x_kv = x\n","\n","        if identity is None:\n","            identity = x_q\n","\n","        # Because the dataflow('key', 'query', 'value') of\n","        # ``torch.nn.MultiheadAttention`` is (num_query, batch,\n","        # embed_dims), We should adjust the shape of dataflow from\n","        # batch_first (batch, num_query, embed_dims) to num_query_first\n","        # (num_query ,batch, embed_dims), and recover ``attn_output``\n","        # from num_query_first to batch_first.\n","        if self.batch_first:\n","            x_q = x_q.transpose(0, 1)\n","            x_kv = x_kv.transpose(0, 1)\n","\n","        out = self.attn(query=x_q, key=x_kv, value=x_kv)[0]\n","\n","        if self.batch_first:\n","            out = out.transpose(0, 1)\n","\n","        return identity + self.dropout_layer(self.proj_drop(out))\n","\n","    def legacy_forward(self, x, hw_shape, identity=None):\n","        \"\"\"multi head attention forward in mmcv version < 1.3.17.\"\"\"\n","\n","        x_q = x\n","        if self.sr_ratio > 1:\n","            x_kv = nlc_to_nchw(x, hw_shape)\n","            x_kv = self.sr(x_kv)\n","            x_kv = nchw_to_nlc(x_kv)\n","            x_kv = self.norm(x_kv)\n","        else:\n","            x_kv = x\n","\n","        if identity is None:\n","            identity = x_q\n","\n","        # `need_weights=True` will let nn.MultiHeadAttention\n","        # `return attn_output, attn_output_weights.sum(dim=1) / num_heads`\n","        # The `attn_output_weights.sum(dim=1)` may cause cuda error. So, we set\n","        # `need_weights=False` to ignore `attn_output_weights.sum(dim=1)`.\n","        # This issue - `https://github.com/pytorch/pytorch/issues/37583` report\n","        # the error that large scale tensor sum operation may cause cuda error.\n","        out = self.attn(query=x_q, key=x_kv, value=x_kv, need_weights=False)[0]\n","\n","        return identity + self.dropout_layer(self.proj_drop(out))\n","\n","\n","class TransformerEncoderLayer(BaseModule):\n","    \"\"\"Implements one encoder layer in Segformer.\n","\n","    Args:\n","        embed_dims (int): The feature dimension.\n","        num_heads (int): Parallel attention heads.\n","        feedforward_channels (int): The hidden dimension for FFNs.\n","        drop_rate (float): Probability of an element to be zeroed.\n","            after the feed forward layer. Default 0.0.\n","        attn_drop_rate (float): The drop out rate for attention layer.\n","            Default 0.0.\n","        drop_path_rate (float): stochastic depth rate. Default 0.0.\n","        qkv_bias (bool): enable bias for qkv if True.\n","            Default: True.\n","        act_cfg (dict): The activation config for FFNs.\n","            Default: dict(type='GELU').\n","        norm_cfg (dict): Config dict for normalization layer.\n","            Default: dict(type='LN').\n","        batch_first (bool): Key, Query and Value are shape of\n","            (batch, n, embed_dim)\n","            or (n, batch, embed_dim). Default: False.\n","        init_cfg (dict, optional): Initialization config dict.\n","            Default:None.\n","        sr_ratio (int): The ratio of spatial reduction of Efficient Multi-head\n","            Attention of Segformer. Default: 1.\n","        with_cp (bool): Use checkpoint or not. Using checkpoint will save\n","            some memory while slowing down the training speed. Default: False.\n","    \"\"\"\n","\n","    def __init__(self,\n","                 embed_dims,\n","                 num_heads,\n","                 feedforward_channels,\n","                 drop_rate=0.,\n","                 attn_drop_rate=0.,\n","                 drop_path_rate=0.,\n","                 qkv_bias=True,\n","                 act_cfg=dict(type='GELU'),\n","                 norm_cfg=dict(type='LN'),\n","                 batch_first=True,\n","                 sr_ratio=1,\n","                 with_cp=False):\n","        super().__init__()\n","\n","        # The ret[0] of build_norm_layer is norm name.\n","        self.norm1 = build_norm_layer(norm_cfg, embed_dims)[1]\n","\n","        self.attn = EfficientMultiheadAttention(\n","            embed_dims=embed_dims,\n","            num_heads=num_heads,\n","            attn_drop=attn_drop_rate,\n","            proj_drop=drop_rate,\n","            dropout_layer=dict(type='DropPath', drop_prob=drop_path_rate),\n","            batch_first=batch_first,\n","            qkv_bias=qkv_bias,\n","            norm_cfg=norm_cfg,\n","            sr_ratio=sr_ratio)\n","\n","        # The ret[0] of build_norm_layer is norm name.\n","        self.norm2 = build_norm_layer(norm_cfg, embed_dims)[1]\n","\n","        self.ffn = MixFFN(\n","            embed_dims=embed_dims,\n","            feedforward_channels=feedforward_channels,\n","            ffn_drop=drop_rate,\n","            dropout_layer=dict(type='DropPath', drop_prob=drop_path_rate),\n","            act_cfg=act_cfg)\n","\n","        self.with_cp = with_cp\n","\n","    def forward(self, x, hw_shape):\n","\n","        def _inner_forward(x):\n","            x = self.attn(self.norm1(x), hw_shape, identity=x)\n","            x = self.ffn(self.norm2(x), hw_shape, identity=x)\n","            return x\n","\n","        if self.with_cp and x.requires_grad:\n","            x = cp.checkpoint(_inner_forward, x)\n","        else:\n","            x = _inner_forward(x)\n","        return x\n","\n","\n","@MODELS.register_module()\n","class CustomMixVisionTransformer(BaseModule):\n","    \"\"\"The backbone of Segformer.\n","\n","    This backbone is the implementation of `SegFormer: Simple and\n","    Efficient Design for Semantic Segmentation with\n","    Transformers <https://arxiv.org/abs/2105.15203>`_.\n","    Args:\n","        in_channels (int): Number of input channels. Default: 3.\n","        embed_dims (int): Embedding dimension. Default: 768.\n","        num_stags (int): The num of stages. Default: 4.\n","        num_layers (Sequence[int]): The layer number of each transformer encode\n","            layer. Default: [3, 4, 6, 3].\n","        num_heads (Sequence[int]): The attention heads of each transformer\n","            encode layer. Default: [1, 2, 4, 8].\n","        patch_sizes (Sequence[int]): The patch_size of each overlapped patch\n","            embedding. Default: [7, 3, 3, 3].\n","        strides (Sequence[int]): The stride of each overlapped patch embedding.\n","            Default: [4, 2, 2, 2].\n","        sr_ratios (Sequence[int]): The spatial reduction rate of each\n","            transformer encode layer. Default: [8, 4, 2, 1].\n","        out_indices (Sequence[int] | int): Output from which stages.\n","            Default: (0, 1, 2, 3).\n","        mlp_ratio (int): ratio of mlp hidden dim to embedding dim.\n","            Default: 4.\n","        qkv_bias (bool): Enable bias for qkv if True. Default: True.\n","        drop_rate (float): Probability of an element to be zeroed.\n","            Default 0.0\n","        attn_drop_rate (float): The drop out rate for attention layer.\n","            Default 0.0\n","        drop_path_rate (float): stochastic depth rate. Default 0.0\n","        norm_cfg (dict): Config dict for normalization layer.\n","            Default: dict(type='LN')\n","        act_cfg (dict): The activation config for FFNs.\n","            Default: dict(type='GELU').\n","        pretrained (str, optional): model pretrained path. Default: None.\n","        init_cfg (dict or list[dict], optional): Initialization config dict.\n","            Default: None.\n","        with_cp (bool): Use checkpoint or not. Using checkpoint will save\n","            some memory while slowing down the training speed. Default: False.\n","    \"\"\"\n","\n","    def __init__(self,\n","                 in_channels=3,\n","                 embed_dims=32,\n","                 num_stages=4,\n","                 num_layers=[2, 2, 2, 2],\n","                 num_heads=[1, 2, 5, 8],\n","                 patch_sizes=[7, 3, 3, 3],\n","                 strides=[4, 2, 2, 2],\n","                 sr_ratios=[8, 4, 2, 1],\n","                 out_indices=(0, 1, 2, 3),\n","                 mlp_ratio=4,\n","                 qkv_bias=True,\n","                 drop_rate=0.,\n","                 attn_drop_rate=0.,\n","                 drop_path_rate=0.1,\n","                 act_cfg=dict(type='GELU'),\n","                 norm_cfg=dict(type='LN', eps=1e-6),\n","                 pretrained=None,\n","                 init_cfg=None,\n","                 with_cp=False):\n","        \n","        super().__init__(init_cfg=init_cfg)\n","\n","        assert not (init_cfg and pretrained), \\\n","            'init_cfg and pretrained cannot be set at the same time'\n","        if isinstance(pretrained, str):\n","            warnings.warn('DeprecationWarning: pretrained is deprecated, '\n","                          'please use \"init_cfg\" instead')\n","            self.init_cfg = dict(type='Pretrained', checkpoint=pretrained)\n","        elif pretrained is not None:\n","            raise TypeError('pretrained must be a str or None')\n","\n","        self.embed_dims = embed_dims\n","        self.num_stages = num_stages\n","        self.num_layers = num_layers\n","        self.num_heads = num_heads\n","        self.patch_sizes = patch_sizes\n","        self.strides = strides\n","        self.sr_ratios = sr_ratios\n","        self.with_cp = with_cp\n","        assert num_stages == len(num_layers) == len(num_heads) \\\n","               == len(patch_sizes) == len(strides) == len(sr_ratios)\n","\n","        self.out_indices = out_indices\n","        assert max(out_indices) < self.num_stages\n","\n","        # Conv 추가\n","        self.downconv = nn.Conv2d(in_channels=3, out_channels=8, kernel_size=3, stride=2, padding=1)\n","\n","        # transformer encoder\n","        dpr = [\n","            x.item()\n","            for x in torch.linspace(0, drop_path_rate, sum(num_layers))\n","        ]  # stochastic num_layer decay rule\n","\n","        cur = 0\n","        self.layers = ModuleList()\n","        for i, num_layer in enumerate(num_layers):\n","            embed_dims_i = embed_dims * num_heads[i]\n","            patch_embed = PatchEmbed(\n","                in_channels=in_channels,\n","                embed_dims=embed_dims_i,\n","                kernel_size=patch_sizes[i],\n","                stride=strides[i],\n","                padding=patch_sizes[i] // 2,\n","                norm_cfg=norm_cfg)\n","            layer = ModuleList([\n","                TransformerEncoderLayer(\n","                    embed_dims=embed_dims_i,\n","                    num_heads=num_heads[i],\n","                    feedforward_channels=mlp_ratio * embed_dims_i,\n","                    drop_rate=drop_rate,\n","                    attn_drop_rate=attn_drop_rate,\n","                    drop_path_rate=dpr[cur + idx],\n","                    qkv_bias=qkv_bias,\n","                    act_cfg=act_cfg,\n","                    norm_cfg=norm_cfg,\n","                    with_cp=with_cp,\n","                    sr_ratio=sr_ratios[i]) for idx in range(num_layer)\n","            ])\n","            in_channels = embed_dims_i\n","            # The ret[0] of build_norm_layer is norm name.\n","            norm = build_norm_layer(norm_cfg, embed_dims_i)[1]\n","            self.layers.append(ModuleList([patch_embed, layer, norm]))\n","            cur += num_layer\n","\n","    def init_weights(self):\n","        if self.init_cfg is None:\n","            for m in self.modules():\n","                if isinstance(m, nn.Linear):\n","                    trunc_normal_init(m, std=.02, bias=0.)\n","                elif isinstance(m, nn.LayerNorm):\n","                    constant_init(m, val=1.0, bias=0.)\n","                elif isinstance(m, nn.Conv2d):\n","                    fan_out = m.kernel_size[0] * m.kernel_size[\n","                        1] * m.out_channels\n","                    fan_out //= m.groups\n","                    normal_init(\n","                        m, mean=0, std=math.sqrt(2.0 / fan_out), bias=0)\n","        else:\n","            super().init_weights()\n","\n","    def forward(self, x):\n","        outs = []\n","\n","        x = self.downconv(x)\n","        for i, layer in enumerate(self.layers):\n","            x, hw_shape = layer[0](x)\n","            for block in layer[1]:\n","                x = block(x, hw_shape)\n","            x = layer[2](x)\n","            x = nlc_to_nchw(x, hw_shape)\n","            if i in self.out_indices:\n","                outs.append(x)\n","\n","        return outs\n"]},{"cell_type":"code","execution_count":18,"id":"c2c97272","metadata":{},"outputs":[],"source":["model = CustomMixVisionTransformer()"]},{"cell_type":"code","execution_count":19,"id":"95009f16","metadata":{},"outputs":[{"data":{"text/plain":["CustomMixVisionTransformer(\n","  (conv): Conv2d(3, 8, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n","  (layers): ModuleList(\n","    (0): ModuleList(\n","      (0): PatchEmbed(\n","        (projection): Conv2d(3, 32, kernel_size=(7, 7), stride=(4, 4), padding=(3, 3))\n","        (norm): LayerNorm((32,), eps=1e-06, elementwise_affine=True)\n","      )\n","      (1): ModuleList(\n","        (0): TransformerEncoderLayer(\n","          (norm1): LayerNorm((32,), eps=1e-06, elementwise_affine=True)\n","          (attn): EfficientMultiheadAttention(\n","            (attn): MultiheadAttention(\n","              (out_proj): NonDynamicallyQuantizableLinear(in_features=32, out_features=32, bias=True)\n","            )\n","            (proj_drop): Dropout(p=0.0, inplace=False)\n","            (dropout_layer): DropPath()\n","            (sr): Conv2d(32, 32, kernel_size=(8, 8), stride=(8, 8))\n","            (norm): LayerNorm((32,), eps=1e-06, elementwise_affine=True)\n","          )\n","          (norm2): LayerNorm((32,), eps=1e-06, elementwise_affine=True)\n","          (ffn): MixFFN(\n","            (activate): GELU(approximate=none)\n","            (layers): Sequential(\n","              (0): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))\n","              (1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128)\n","              (2): GELU(approximate=none)\n","              (3): Dropout(p=0.0, inplace=False)\n","              (4): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))\n","              (5): Dropout(p=0.0, inplace=False)\n","            )\n","            (dropout_layer): DropPath()\n","          )\n","        )\n","        (1): TransformerEncoderLayer(\n","          (norm1): LayerNorm((32,), eps=1e-06, elementwise_affine=True)\n","          (attn): EfficientMultiheadAttention(\n","            (attn): MultiheadAttention(\n","              (out_proj): NonDynamicallyQuantizableLinear(in_features=32, out_features=32, bias=True)\n","            )\n","            (proj_drop): Dropout(p=0.0, inplace=False)\n","            (dropout_layer): DropPath()\n","            (sr): Conv2d(32, 32, kernel_size=(8, 8), stride=(8, 8))\n","            (norm): LayerNorm((32,), eps=1e-06, elementwise_affine=True)\n","          )\n","          (norm2): LayerNorm((32,), eps=1e-06, elementwise_affine=True)\n","          (ffn): MixFFN(\n","            (activate): GELU(approximate=none)\n","            (layers): Sequential(\n","              (0): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))\n","              (1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128)\n","              (2): GELU(approximate=none)\n","              (3): Dropout(p=0.0, inplace=False)\n","              (4): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))\n","              (5): Dropout(p=0.0, inplace=False)\n","            )\n","            (dropout_layer): DropPath()\n","          )\n","        )\n","      )\n","      (2): LayerNorm((32,), eps=1e-06, elementwise_affine=True)\n","    )\n","    (1): ModuleList(\n","      (0): PatchEmbed(\n","        (projection): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n","        (norm): LayerNorm((64,), eps=1e-06, elementwise_affine=True)\n","      )\n","      (1): ModuleList(\n","        (0): TransformerEncoderLayer(\n","          (norm1): LayerNorm((64,), eps=1e-06, elementwise_affine=True)\n","          (attn): EfficientMultiheadAttention(\n","            (attn): MultiheadAttention(\n","              (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)\n","            )\n","            (proj_drop): Dropout(p=0.0, inplace=False)\n","            (dropout_layer): DropPath()\n","            (sr): Conv2d(64, 64, kernel_size=(4, 4), stride=(4, 4))\n","            (norm): LayerNorm((64,), eps=1e-06, elementwise_affine=True)\n","          )\n","          (norm2): LayerNorm((64,), eps=1e-06, elementwise_affine=True)\n","          (ffn): MixFFN(\n","            (activate): GELU(approximate=none)\n","            (layers): Sequential(\n","              (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))\n","              (1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)\n","              (2): GELU(approximate=none)\n","              (3): Dropout(p=0.0, inplace=False)\n","              (4): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))\n","              (5): Dropout(p=0.0, inplace=False)\n","            )\n","            (dropout_layer): DropPath()\n","          )\n","        )\n","        (1): TransformerEncoderLayer(\n","          (norm1): LayerNorm((64,), eps=1e-06, elementwise_affine=True)\n","          (attn): EfficientMultiheadAttention(\n","            (attn): MultiheadAttention(\n","              (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)\n","            )\n","            (proj_drop): Dropout(p=0.0, inplace=False)\n","            (dropout_layer): DropPath()\n","            (sr): Conv2d(64, 64, kernel_size=(4, 4), stride=(4, 4))\n","            (norm): LayerNorm((64,), eps=1e-06, elementwise_affine=True)\n","          )\n","          (norm2): LayerNorm((64,), eps=1e-06, elementwise_affine=True)\n","          (ffn): MixFFN(\n","            (activate): GELU(approximate=none)\n","            (layers): Sequential(\n","              (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))\n","              (1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)\n","              (2): GELU(approximate=none)\n","              (3): Dropout(p=0.0, inplace=False)\n","              (4): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))\n","              (5): Dropout(p=0.0, inplace=False)\n","            )\n","            (dropout_layer): DropPath()\n","          )\n","        )\n","      )\n","      (2): LayerNorm((64,), eps=1e-06, elementwise_affine=True)\n","    )\n","    (2): ModuleList(\n","      (0): PatchEmbed(\n","        (projection): Conv2d(64, 160, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n","        (norm): LayerNorm((160,), eps=1e-06, elementwise_affine=True)\n","      )\n","      (1): ModuleList(\n","        (0): TransformerEncoderLayer(\n","          (norm1): LayerNorm((160,), eps=1e-06, elementwise_affine=True)\n","          (attn): EfficientMultiheadAttention(\n","            (attn): MultiheadAttention(\n","              (out_proj): NonDynamicallyQuantizableLinear(in_features=160, out_features=160, bias=True)\n","            )\n","            (proj_drop): Dropout(p=0.0, inplace=False)\n","            (dropout_layer): DropPath()\n","            (sr): Conv2d(160, 160, kernel_size=(2, 2), stride=(2, 2))\n","            (norm): LayerNorm((160,), eps=1e-06, elementwise_affine=True)\n","          )\n","          (norm2): LayerNorm((160,), eps=1e-06, elementwise_affine=True)\n","          (ffn): MixFFN(\n","            (activate): GELU(approximate=none)\n","            (layers): Sequential(\n","              (0): Conv2d(160, 640, kernel_size=(1, 1), stride=(1, 1))\n","              (1): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=640)\n","              (2): GELU(approximate=none)\n","              (3): Dropout(p=0.0, inplace=False)\n","              (4): Conv2d(640, 160, kernel_size=(1, 1), stride=(1, 1))\n","              (5): Dropout(p=0.0, inplace=False)\n","            )\n","            (dropout_layer): DropPath()\n","          )\n","        )\n","        (1): TransformerEncoderLayer(\n","          (norm1): LayerNorm((160,), eps=1e-06, elementwise_affine=True)\n","          (attn): EfficientMultiheadAttention(\n","            (attn): MultiheadAttention(\n","              (out_proj): NonDynamicallyQuantizableLinear(in_features=160, out_features=160, bias=True)\n","            )\n","            (proj_drop): Dropout(p=0.0, inplace=False)\n","            (dropout_layer): DropPath()\n","            (sr): Conv2d(160, 160, kernel_size=(2, 2), stride=(2, 2))\n","            (norm): LayerNorm((160,), eps=1e-06, elementwise_affine=True)\n","          )\n","          (norm2): LayerNorm((160,), eps=1e-06, elementwise_affine=True)\n","          (ffn): MixFFN(\n","            (activate): GELU(approximate=none)\n","            (layers): Sequential(\n","              (0): Conv2d(160, 640, kernel_size=(1, 1), stride=(1, 1))\n","              (1): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=640)\n","              (2): GELU(approximate=none)\n","              (3): Dropout(p=0.0, inplace=False)\n","              (4): Conv2d(640, 160, kernel_size=(1, 1), stride=(1, 1))\n","              (5): Dropout(p=0.0, inplace=False)\n","            )\n","            (dropout_layer): DropPath()\n","          )\n","        )\n","      )\n","      (2): LayerNorm((160,), eps=1e-06, elementwise_affine=True)\n","    )\n","    (3): ModuleList(\n","      (0): PatchEmbed(\n","        (projection): Conv2d(160, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n","        (norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)\n","      )\n","      (1): ModuleList(\n","        (0): TransformerEncoderLayer(\n","          (norm1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)\n","          (attn): EfficientMultiheadAttention(\n","            (attn): MultiheadAttention(\n","              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n","            )\n","            (proj_drop): Dropout(p=0.0, inplace=False)\n","            (dropout_layer): DropPath()\n","          )\n","          (norm2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)\n","          (ffn): MixFFN(\n","            (activate): GELU(approximate=none)\n","            (layers): Sequential(\n","              (0): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1))\n","              (1): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024)\n","              (2): GELU(approximate=none)\n","              (3): Dropout(p=0.0, inplace=False)\n","              (4): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n","              (5): Dropout(p=0.0, inplace=False)\n","            )\n","            (dropout_layer): DropPath()\n","          )\n","        )\n","        (1): TransformerEncoderLayer(\n","          (norm1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)\n","          (attn): EfficientMultiheadAttention(\n","            (attn): MultiheadAttention(\n","              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n","            )\n","            (proj_drop): Dropout(p=0.0, inplace=False)\n","            (dropout_layer): DropPath()\n","          )\n","          (norm2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)\n","          (ffn): MixFFN(\n","            (activate): GELU(approximate=none)\n","            (layers): Sequential(\n","              (0): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1))\n","              (1): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024)\n","              (2): GELU(approximate=none)\n","              (3): Dropout(p=0.0, inplace=False)\n","              (4): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n","              (5): Dropout(p=0.0, inplace=False)\n","            )\n","            (dropout_layer): DropPath()\n","          )\n","        )\n","      )\n","      (2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)\n","    )\n","  )\n",")"]},"execution_count":19,"metadata":{},"output_type":"execute_result"}],"source":["model"]},{"cell_type":"code","execution_count":20,"id":"3f4992fa","metadata":{},"outputs":[],"source":["input = torch.randn(1, 3, 2048, 2048)"]},{"cell_type":"code","execution_count":21,"id":"7dc30dde","metadata":{},"outputs":[],"source":["conv = nn.Conv2d(in_channels=3, out_channels=8, kernel_size=3, stride=2, padding=1)"]},{"cell_type":"code","execution_count":22,"id":"34993b9d","metadata":{},"outputs":[],"source":["output = conv(input)"]},{"cell_type":"code","execution_count":23,"id":"b719a29a","metadata":{},"outputs":[{"data":{"text/plain":["torch.Size([1, 8, 1024, 1024])"]},"execution_count":23,"metadata":{},"output_type":"execute_result"}],"source":["output.shape"]},{"cell_type":"code","execution_count":24,"id":"a591e155","metadata":{},"outputs":[],"source":["model.layers[0][0].projection = Conv2d(8, 32, kernel_size=(7, 7), stride=(4, 4), padding=(3, 3))"]},{"cell_type":"code","execution_count":25,"id":"f4ec3d90","metadata":{},"outputs":[{"data":{"text/plain":["ModuleList(\n","  (0): ModuleList(\n","    (0): PatchEmbed(\n","      (projection): Conv2d(8, 32, kernel_size=(7, 7), stride=(4, 4), padding=(3, 3))\n","      (norm): LayerNorm((32,), eps=1e-06, elementwise_affine=True)\n","    )\n","    (1): ModuleList(\n","      (0): TransformerEncoderLayer(\n","        (norm1): LayerNorm((32,), eps=1e-06, elementwise_affine=True)\n","        (attn): EfficientMultiheadAttention(\n","          (attn): MultiheadAttention(\n","            (out_proj): NonDynamicallyQuantizableLinear(in_features=32, out_features=32, bias=True)\n","          )\n","          (proj_drop): Dropout(p=0.0, inplace=False)\n","          (dropout_layer): DropPath()\n","          (sr): Conv2d(32, 32, kernel_size=(8, 8), stride=(8, 8))\n","          (norm): LayerNorm((32,), eps=1e-06, elementwise_affine=True)\n","        )\n","        (norm2): LayerNorm((32,), eps=1e-06, elementwise_affine=True)\n","        (ffn): MixFFN(\n","          (activate): GELU(approximate=none)\n","          (layers): Sequential(\n","            (0): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))\n","            (1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128)\n","            (2): GELU(approximate=none)\n","            (3): Dropout(p=0.0, inplace=False)\n","            (4): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))\n","            (5): Dropout(p=0.0, inplace=False)\n","          )\n","          (dropout_layer): DropPath()\n","        )\n","      )\n","      (1): TransformerEncoderLayer(\n","        (norm1): LayerNorm((32,), eps=1e-06, elementwise_affine=True)\n","        (attn): EfficientMultiheadAttention(\n","          (attn): MultiheadAttention(\n","            (out_proj): NonDynamicallyQuantizableLinear(in_features=32, out_features=32, bias=True)\n","          )\n","          (proj_drop): Dropout(p=0.0, inplace=False)\n","          (dropout_layer): DropPath()\n","          (sr): Conv2d(32, 32, kernel_size=(8, 8), stride=(8, 8))\n","          (norm): LayerNorm((32,), eps=1e-06, elementwise_affine=True)\n","        )\n","        (norm2): LayerNorm((32,), eps=1e-06, elementwise_affine=True)\n","        (ffn): MixFFN(\n","          (activate): GELU(approximate=none)\n","          (layers): Sequential(\n","            (0): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))\n","            (1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128)\n","            (2): GELU(approximate=none)\n","            (3): Dropout(p=0.0, inplace=False)\n","            (4): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))\n","            (5): Dropout(p=0.0, inplace=False)\n","          )\n","          (dropout_layer): DropPath()\n","        )\n","      )\n","    )\n","    (2): LayerNorm((32,), eps=1e-06, elementwise_affine=True)\n","  )\n","  (1): ModuleList(\n","    (0): PatchEmbed(\n","      (projection): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n","      (norm): LayerNorm((64,), eps=1e-06, elementwise_affine=True)\n","    )\n","    (1): ModuleList(\n","      (0): TransformerEncoderLayer(\n","        (norm1): LayerNorm((64,), eps=1e-06, elementwise_affine=True)\n","        (attn): EfficientMultiheadAttention(\n","          (attn): MultiheadAttention(\n","            (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)\n","          )\n","          (proj_drop): Dropout(p=0.0, inplace=False)\n","          (dropout_layer): DropPath()\n","          (sr): Conv2d(64, 64, kernel_size=(4, 4), stride=(4, 4))\n","          (norm): LayerNorm((64,), eps=1e-06, elementwise_affine=True)\n","        )\n","        (norm2): LayerNorm((64,), eps=1e-06, elementwise_affine=True)\n","        (ffn): MixFFN(\n","          (activate): GELU(approximate=none)\n","          (layers): Sequential(\n","            (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))\n","            (1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)\n","            (2): GELU(approximate=none)\n","            (3): Dropout(p=0.0, inplace=False)\n","            (4): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))\n","            (5): Dropout(p=0.0, inplace=False)\n","          )\n","          (dropout_layer): DropPath()\n","        )\n","      )\n","      (1): TransformerEncoderLayer(\n","        (norm1): LayerNorm((64,), eps=1e-06, elementwise_affine=True)\n","        (attn): EfficientMultiheadAttention(\n","          (attn): MultiheadAttention(\n","            (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)\n","          )\n","          (proj_drop): Dropout(p=0.0, inplace=False)\n","          (dropout_layer): DropPath()\n","          (sr): Conv2d(64, 64, kernel_size=(4, 4), stride=(4, 4))\n","          (norm): LayerNorm((64,), eps=1e-06, elementwise_affine=True)\n","        )\n","        (norm2): LayerNorm((64,), eps=1e-06, elementwise_affine=True)\n","        (ffn): MixFFN(\n","          (activate): GELU(approximate=none)\n","          (layers): Sequential(\n","            (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))\n","            (1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)\n","            (2): GELU(approximate=none)\n","            (3): Dropout(p=0.0, inplace=False)\n","            (4): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))\n","            (5): Dropout(p=0.0, inplace=False)\n","          )\n","          (dropout_layer): DropPath()\n","        )\n","      )\n","    )\n","    (2): LayerNorm((64,), eps=1e-06, elementwise_affine=True)\n","  )\n","  (2): ModuleList(\n","    (0): PatchEmbed(\n","      (projection): Conv2d(64, 160, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n","      (norm): LayerNorm((160,), eps=1e-06, elementwise_affine=True)\n","    )\n","    (1): ModuleList(\n","      (0): TransformerEncoderLayer(\n","        (norm1): LayerNorm((160,), eps=1e-06, elementwise_affine=True)\n","        (attn): EfficientMultiheadAttention(\n","          (attn): MultiheadAttention(\n","            (out_proj): NonDynamicallyQuantizableLinear(in_features=160, out_features=160, bias=True)\n","          )\n","          (proj_drop): Dropout(p=0.0, inplace=False)\n","          (dropout_layer): DropPath()\n","          (sr): Conv2d(160, 160, kernel_size=(2, 2), stride=(2, 2))\n","          (norm): LayerNorm((160,), eps=1e-06, elementwise_affine=True)\n","        )\n","        (norm2): LayerNorm((160,), eps=1e-06, elementwise_affine=True)\n","        (ffn): MixFFN(\n","          (activate): GELU(approximate=none)\n","          (layers): Sequential(\n","            (0): Conv2d(160, 640, kernel_size=(1, 1), stride=(1, 1))\n","            (1): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=640)\n","            (2): GELU(approximate=none)\n","            (3): Dropout(p=0.0, inplace=False)\n","            (4): Conv2d(640, 160, kernel_size=(1, 1), stride=(1, 1))\n","            (5): Dropout(p=0.0, inplace=False)\n","          )\n","          (dropout_layer): DropPath()\n","        )\n","      )\n","      (1): TransformerEncoderLayer(\n","        (norm1): LayerNorm((160,), eps=1e-06, elementwise_affine=True)\n","        (attn): EfficientMultiheadAttention(\n","          (attn): MultiheadAttention(\n","            (out_proj): NonDynamicallyQuantizableLinear(in_features=160, out_features=160, bias=True)\n","          )\n","          (proj_drop): Dropout(p=0.0, inplace=False)\n","          (dropout_layer): DropPath()\n","          (sr): Conv2d(160, 160, kernel_size=(2, 2), stride=(2, 2))\n","          (norm): LayerNorm((160,), eps=1e-06, elementwise_affine=True)\n","        )\n","        (norm2): LayerNorm((160,), eps=1e-06, elementwise_affine=True)\n","        (ffn): MixFFN(\n","          (activate): GELU(approximate=none)\n","          (layers): Sequential(\n","            (0): Conv2d(160, 640, kernel_size=(1, 1), stride=(1, 1))\n","            (1): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=640)\n","            (2): GELU(approximate=none)\n","            (3): Dropout(p=0.0, inplace=False)\n","            (4): Conv2d(640, 160, kernel_size=(1, 1), stride=(1, 1))\n","            (5): Dropout(p=0.0, inplace=False)\n","          )\n","          (dropout_layer): DropPath()\n","        )\n","      )\n","    )\n","    (2): LayerNorm((160,), eps=1e-06, elementwise_affine=True)\n","  )\n","  (3): ModuleList(\n","    (0): PatchEmbed(\n","      (projection): Conv2d(160, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n","      (norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)\n","    )\n","    (1): ModuleList(\n","      (0): TransformerEncoderLayer(\n","        (norm1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)\n","        (attn): EfficientMultiheadAttention(\n","          (attn): MultiheadAttention(\n","            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n","          )\n","          (proj_drop): Dropout(p=0.0, inplace=False)\n","          (dropout_layer): DropPath()\n","        )\n","        (norm2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)\n","        (ffn): MixFFN(\n","          (activate): GELU(approximate=none)\n","          (layers): Sequential(\n","            (0): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1))\n","            (1): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024)\n","            (2): GELU(approximate=none)\n","            (3): Dropout(p=0.0, inplace=False)\n","            (4): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n","            (5): Dropout(p=0.0, inplace=False)\n","          )\n","          (dropout_layer): DropPath()\n","        )\n","      )\n","      (1): TransformerEncoderLayer(\n","        (norm1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)\n","        (attn): EfficientMultiheadAttention(\n","          (attn): MultiheadAttention(\n","            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n","          )\n","          (proj_drop): Dropout(p=0.0, inplace=False)\n","          (dropout_layer): DropPath()\n","        )\n","        (norm2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)\n","        (ffn): MixFFN(\n","          (activate): GELU(approximate=none)\n","          (layers): Sequential(\n","            (0): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1))\n","            (1): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024)\n","            (2): GELU(approximate=none)\n","            (3): Dropout(p=0.0, inplace=False)\n","            (4): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n","            (5): Dropout(p=0.0, inplace=False)\n","          )\n","          (dropout_layer): DropPath()\n","        )\n","      )\n","    )\n","    (2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)\n","  )\n",")"]},"execution_count":25,"metadata":{},"output_type":"execute_result"}],"source":["model.layers"]},{"cell_type":"code","execution_count":26,"id":"de70fd15","metadata":{},"outputs":[],"source":["output = model(input)"]},{"cell_type":"code","execution_count":27,"id":"a80241b8","metadata":{},"outputs":[{"data":{"text/plain":["4"]},"execution_count":27,"metadata":{},"output_type":"execute_result"}],"source":["len(output)"]},{"cell_type":"code","execution_count":28,"id":"1b7a6cd5","metadata":{},"outputs":[],"source":["# Copyright (c) OpenMMLab. All rights reserved.\n","import warnings\n","from abc import ABCMeta, abstractmethod\n","from typing import List, Tuple\n","\n","import torch\n","import torch.nn as nn\n","from mmengine.model import BaseModule\n","from torch import Tensor\n","\n","from mmseg.registry import MODELS\n","from mmseg.structures import build_pixel_sampler\n","from mmseg.utils import ConfigType, SampleList\n","from mmseg.models.losses import accuracy\n","from mmseg.models.utils import resize\n","\n","\n","class BaseDecodeHead(BaseModule, metaclass=ABCMeta):\n","    \"\"\"Base class for BaseDecodeHead.\n","\n","    1. The ``init_weights`` method is used to initialize decode_head's\n","    model parameters. After segmentor initialization, ``init_weights``\n","    is triggered when ``segmentor.init_weights()`` is called externally.\n","\n","    2. The ``loss`` method is used to calculate the loss of decode_head,\n","    which includes two steps: (1) the decode_head model performs forward\n","    propagation to obtain the feature maps (2) The ``loss_by_feat`` method\n","    is called based on the feature maps to calculate the loss.\n","\n","    .. code:: text\n","\n","    loss(): forward() -> loss_by_feat()\n","\n","    3. The ``predict`` method is used to predict segmentation results,\n","    which includes two steps: (1) the decode_head model performs forward\n","    propagation to obtain the feature maps (2) The ``predict_by_feat`` method\n","    is called based on the feature maps to predict segmentation results\n","    including post-processing.\n","\n","    .. code:: text\n","\n","    predict(): forward() -> predict_by_feat()\n","\n","    Args:\n","        in_channels (int|Sequence[int]): Input channels.\n","        channels (int): Channels after modules, before conv_seg.\n","        num_classes (int): Number of classes.\n","        out_channels (int): Output channels of conv_seg. Default: None.\n","        threshold (float): Threshold for binary segmentation in the case of\n","            `num_classes==1`. Default: None.\n","        dropout_ratio (float): Ratio of dropout layer. Default: 0.1.\n","        conv_cfg (dict|None): Config of conv layers. Default: None.\n","        norm_cfg (dict|None): Config of norm layers. Default: None.\n","        act_cfg (dict): Config of activation layers.\n","            Default: dict(type='ReLU')\n","        in_index (int|Sequence[int]): Input feature index. Default: -1\n","        input_transform (str|None): Transformation type of input features.\n","            Options: 'resize_concat', 'multiple_select', None.\n","            'resize_concat': Multiple feature maps will be resize to the\n","                same size as first one and than concat together.\n","                Usually used in FCN head of HRNet.\n","            'multiple_select': Multiple feature maps will be bundle into\n","                a list and passed into decode head.\n","            None: Only one select feature map is allowed.\n","            Default: None.\n","        loss_decode (dict | Sequence[dict]): Config of decode loss.\n","            The `loss_name` is property of corresponding loss function which\n","            could be shown in training log. If you want this loss\n","            item to be included into the backward graph, `loss_` must be the\n","            prefix of the name. Defaults to 'loss_ce'.\n","             e.g. dict(type='CrossEntropyLoss'),\n","             [dict(type='CrossEntropyLoss', loss_name='loss_ce'),\n","              dict(type='DiceLoss', loss_name='loss_dice')]\n","            Default: dict(type='CrossEntropyLoss').\n","        ignore_index (int | None): The label index to be ignored. When using\n","            masked BCE loss, ignore_index should be set to None. Default: 255.\n","        sampler (dict|None): The config of segmentation map sampler.\n","            Default: None.\n","        align_corners (bool): align_corners argument of F.interpolate.\n","            Default: False.\n","        init_cfg (dict or list[dict], optional): Initialization config dict.\n","    \"\"\"\n","\n","    norm_cfg = dict(type='SyncBN', requires_grad=True)\n","\n","    def __init__(self,\n","                 in_channels,\n","                 channels,\n","                 *,\n","                 num_classes,\n","                 out_channels=None,\n","                 threshold=None,\n","                 dropout_ratio=0.1,\n","                 conv_cfg=None,\n","                 norm_cfg=None,\n","                 act_cfg=dict(type='ReLU'),\n","                 in_index=[0, 1, 2, 3],\n","                 input_transform=None,\n","                 loss_decode=dict(\n","                     type='CrossEntropyLoss',\n","                     use_sigmoid=False,\n","                     loss_weight=1.0),\n","                 ignore_index=255,\n","                 sampler=None,\n","                 align_corners=False,\n","                 init_cfg=dict(\n","                     type='Normal', std=0.01, override=dict(name='conv_seg'))):\n","        super().__init__(init_cfg)\n","        self._init_inputs(in_channels, in_index, input_transform)\n","        self.channels = channels\n","        self.dropout_ratio = dropout_ratio\n","        self.conv_cfg = conv_cfg\n","        self.norm_cfg = norm_cfg\n","        self.act_cfg = act_cfg\n","        self.in_index = in_index\n","\n","        self.ignore_index = ignore_index\n","        self.align_corners = align_corners\n","\n","        if out_channels is None:\n","            if num_classes == 2:\n","                warnings.warn('For binary segmentation, we suggest using'\n","                              '`out_channels = 1` to define the output'\n","                              'channels of segmentor, and use `threshold`'\n","                              'to convert `seg_logits` into a prediction'\n","                              'applying a threshold')\n","            out_channels = num_classes\n","\n","        if out_channels != num_classes and out_channels != 1:\n","            raise ValueError(\n","                'out_channels should be equal to num_classes,'\n","                'except binary segmentation set out_channels == 1 and'\n","                f'num_classes == 2, but got out_channels={out_channels}'\n","                f'and num_classes={num_classes}')\n","\n","        if out_channels == 1 and threshold is None:\n","            threshold = 0.3\n","            warnings.warn('threshold is not defined for binary, and defaults'\n","                          'to 0.3')\n","        self.num_classes = num_classes\n","        self.out_channels = out_channels\n","        self.threshold = threshold\n","\n","        if isinstance(loss_decode, dict):\n","            self.loss_decode = MODELS.build(loss_decode)\n","        elif isinstance(loss_decode, (list, tuple)):\n","            self.loss_decode = nn.ModuleList()\n","            for loss in loss_decode:\n","                self.loss_decode.append(MODELS.build(loss))\n","        else:\n","            raise TypeError(f'loss_decode must be a dict or sequence of dict,\\\n","                but got {type(loss_decode)}')\n","\n","        if sampler is not None:\n","            self.sampler = build_pixel_sampler(sampler, context=self)\n","        else:\n","            self.sampler = None\n","\n","        self.conv_seg = nn.Conv2d(channels, self.out_channels, kernel_size=1)\n","        if dropout_ratio > 0:\n","            self.dropout = nn.Dropout2d(dropout_ratio)\n","        else:\n","            self.dropout = None\n","\n","    def extra_repr(self):\n","        \"\"\"Extra repr.\"\"\"\n","        s = f'input_transform={self.input_transform}, ' \\\n","            f'ignore_index={self.ignore_index}, ' \\\n","            f'align_corners={self.align_corners}'\n","        return s\n","\n","    def _init_inputs(self, in_channels, in_index, input_transform):\n","        \"\"\"Check and initialize input transforms.\n","\n","        The in_channels, in_index and input_transform must match.\n","        Specifically, when input_transform is None, only single feature map\n","        will be selected. So in_channels and in_index must be of type int.\n","        When input_transform\n","\n","        Args:\n","            in_channels (int|Sequence[int]): Input channels.\n","            in_index (int|Sequence[int]): Input feature index.\n","            input_transform (str|None): Transformation type of input features.\n","                Options: 'resize_concat', 'multiple_select', None.\n","                'resize_concat': Multiple feature maps will be resize to the\n","                    same size as first one and than concat together.\n","                    Usually used in FCN head of HRNet.\n","                'multiple_select': Multiple feature maps will be bundle into\n","                    a list and passed into decode head.\n","                None: Only one select feature map is allowed.\n","        \"\"\"\n","\n","        if input_transform is not None:\n","            assert input_transform in ['resize_concat', 'multiple_select']\n","        self.input_transform = input_transform\n","        self.in_index = in_index\n","        if input_transform is not None:\n","            assert isinstance(in_channels, (list, tuple))\n","            assert isinstance(in_index, (list, tuple))\n","            assert len(in_channels) == len(in_index)\n","            if input_transform == 'resize_concat':\n","                self.in_channels = sum(in_channels)\n","            else:\n","                self.in_channels = in_channels\n","        else:\n","            assert isinstance(in_channels, int)\n","            assert isinstance(in_index, int)\n","            self.in_channels = in_channels\n","\n","    def _transform_inputs(self, inputs):\n","        \"\"\"Transform inputs for decoder.\n","\n","        Args:\n","            inputs (list[Tensor]): List of multi-level img features.\n","\n","        Returns:\n","            Tensor: The transformed inputs\n","        \"\"\"\n","\n","        if self.input_transform == 'resize_concat':\n","            inputs = [inputs[i] for i in self.in_index]\n","            upsampled_inputs = [\n","                resize(\n","                    input=x,\n","                    size=inputs[0].shape[2:],\n","                    mode='bilinear',\n","                    align_corners=self.align_corners) for x in inputs\n","            ]\n","            inputs = torch.cat(upsampled_inputs, dim=1)\n","        elif self.input_transform == 'multiple_select':\n","            inputs = [inputs[i] for i in self.in_index]\n","        else:\n","            inputs = inputs[self.in_index]\n","\n","        return inputs\n","\n","    @abstractmethod\n","    def forward(self, inputs):\n","        \"\"\"Placeholder of forward function.\"\"\"\n","        pass\n","\n","    def cls_seg(self, feat):\n","        \"\"\"Classify each pixel.\"\"\"\n","        if self.dropout is not None:\n","            feat = self.dropout(feat)\n","        output = self.conv_seg(feat)\n","        return output\n","\n","    def loss(self, inputs: Tuple[Tensor], batch_data_samples: SampleList,\n","             train_cfg: ConfigType) -> dict:\n","        \"\"\"Forward function for training.\n","\n","        Args:\n","            inputs (Tuple[Tensor]): List of multi-level img features.\n","            batch_data_samples (list[:obj:`SegDataSample`]): The seg\n","                data samples. It usually includes information such\n","                as `img_metas` or `gt_semantic_seg`.\n","            train_cfg (dict): The training config.\n","\n","        Returns:\n","            dict[str, Tensor]: a dictionary of loss components\n","        \"\"\"\n","        seg_logits = self.forward(inputs)\n","        losses = self.loss_by_feat(seg_logits, batch_data_samples)\n","        return losses\n","\n","    def predict(self, inputs: Tuple[Tensor], batch_img_metas: List[dict],\n","                test_cfg: ConfigType) -> Tensor:\n","        \"\"\"Forward function for prediction.\n","\n","        Args:\n","            inputs (Tuple[Tensor]): List of multi-level img features.\n","            batch_img_metas (dict): List Image info where each dict may also\n","                contain: 'img_shape', 'scale_factor', 'flip', 'img_path',\n","                'ori_shape', and 'pad_shape'.\n","                For details on the values of these keys see\n","                `mmseg/datasets/pipelines/formatting.py:PackSegInputs`.\n","            test_cfg (dict): The testing config.\n","\n","        Returns:\n","            Tensor: Outputs segmentation logits map.\n","        \"\"\"\n","        seg_logits = self.forward(inputs)\n","\n","        return self.predict_by_feat(seg_logits, batch_img_metas)\n","\n","    def _stack_batch_gt(self, batch_data_samples: SampleList) -> Tensor:\n","        gt_semantic_segs = [\n","            data_sample.gt_sem_seg.data for data_sample in batch_data_samples\n","        ]\n","        return torch.stack(gt_semantic_segs, dim=0)\n","\n","    def loss_by_feat(self, seg_logits: Tensor,\n","                     batch_data_samples: SampleList) -> dict:\n","        \"\"\"Compute segmentation loss.\n","\n","        Args:\n","            seg_logits (Tensor): The output from decode head forward function.\n","            batch_data_samples (List[:obj:`SegDataSample`]): The seg\n","                data samples. It usually includes information such\n","                as `metainfo` and `gt_sem_seg`.\n","\n","        Returns:\n","            dict[str, Tensor]: a dictionary of loss components\n","        \"\"\"\n","\n","        seg_label = self._stack_batch_gt(batch_data_samples)\n","        loss = dict()\n","        seg_logits = resize(\n","            input=seg_logits,\n","            size=seg_label.shape[2:],\n","            mode='bilinear',\n","            align_corners=self.align_corners)\n","        if self.sampler is not None:\n","            seg_weight = self.sampler.sample(seg_logits, seg_label)\n","        else:\n","            seg_weight = None\n","        seg_label = seg_label.squeeze(1)\n","\n","        if not isinstance(self.loss_decode, nn.ModuleList):\n","            losses_decode = [self.loss_decode]\n","        else:\n","            losses_decode = self.loss_decode\n","        for loss_decode in losses_decode:\n","            if loss_decode.loss_name not in loss:\n","                loss[loss_decode.loss_name] = loss_decode(\n","                    seg_logits,\n","                    seg_label,\n","                    weight=seg_weight,\n","                    ignore_index=self.ignore_index)\n","            else:\n","                loss[loss_decode.loss_name] += loss_decode(\n","                    seg_logits,\n","                    seg_label,\n","                    weight=seg_weight,\n","                    ignore_index=self.ignore_index)\n","\n","        # print('seg_logits:',seg_logits.shape)\n","        # print('seg_label:',seg_label.shape)\n","\n","        loss['acc_seg'] = accuracy(\n","            seg_logits, seg_label, ignore_index=self.ignore_index)\n","        return loss\n","\n","    def predict_by_feat(self, seg_logits: Tensor,\n","                        batch_img_metas: List[dict]) -> Tensor:\n","        \"\"\"Transform a batch of output seg_logits to the input shape.\n","\n","        Args:\n","            seg_logits (Tensor): The output from decode head forward function.\n","            batch_img_metas (list[dict]): Meta information of each image, e.g.,\n","                image size, scaling factor, etc.\n","\n","        Returns:\n","            Tensor: Outputs segmentation logits map.\n","        \"\"\"\n","\n","        if isinstance(batch_img_metas[0]['img_shape'], torch.Size):\n","            # slide inference\n","            size = batch_img_metas[0]['img_shape']\n","        elif 'pad_shape' in batch_img_metas[0]:\n","            size = batch_img_metas[0]['pad_shape'][:2]\n","        else:\n","            size = batch_img_metas[0]['img_shape']\n","\n","        seg_logits = resize(\n","            input=seg_logits,\n","            size=size,\n","            mode='bilinear',\n","            align_corners=self.align_corners)\n","        return seg_logits\n"]},{"cell_type":"code","execution_count":57,"id":"11aa9164","metadata":{},"outputs":[],"source":["# Copyright (c) OpenMMLab. All rights reserved.\n","import torch\n","import torch.nn as nn\n","from mmcv.cnn import ConvModule\n","\n","from mmseg.models.decode_heads.decode_head import BaseDecodeHead\n","from mmseg.registry import MODELS\n","from mmseg.models.utils import resize\n","\n","\n","# @MODELS.register_module()\n","class CustomSegformerHead(BaseDecodeHead):\n","    \"\"\"The all mlp Head of segformer.\n","\n","    This head is the implementation of\n","    `Segformer <https://arxiv.org/abs/2105.15203>` _.\n","\n","    Args:\n","        interpolate_mode: The interpolate mode of MLP head upsample operation.\n","            Default: 'bilinear'.\n","    \"\"\"\n","    # norm_cfg = dict(type='SyncBN', requires_grad=True)\n","\n","    \n","    def __init__(self, interpolate_mode='bilinear', **kwargs):\n","        super().__init__(input_transform='multiple_select', **kwargs)\n","\n","        self.interpolate_mode = interpolate_mode\n","        num_inputs = len(self.in_channels)\n","\n","        assert num_inputs == len(self.in_index)\n","\n","        self.convs = nn.ModuleList()\n","        for i in range(num_inputs):\n","            self.convs.append(\n","                ConvModule(\n","                    in_channels=self.in_channels[i],\n","                    out_channels=self.channels,\n","                    kernel_size=1,\n","                    stride=1,\n","                    norm_cfg=self.norm_cfg,\n","                    act_cfg=self.act_cfg))\n","\n","        self.fusion_conv = ConvModule(\n","            in_channels=self.channels * num_inputs,\n","            out_channels=self.channels,\n","            kernel_size=1,\n","            norm_cfg=self.norm_cfg)\n","        \n","        # 추가\n","        self.upconv = nn.ConvTranspose2d(in_channels=29, out_channels=29, kernel_size=16, stride=8, padding=4)\n","        \n","\n","    def forward(self, inputs):\n","        # Receive 4 stage backbone feature map: 1/4, 1/8, 1/16, 1/32\n","        inputs = self._transform_inputs(inputs)\n","        outs = []\n","        for idx in range(len(inputs)):\n","            x = inputs[idx]\n","            conv = self.convs[idx]\n","            outs.append(\n","                resize(\n","                    input=conv(x),\n","                    size=inputs[0].shape[2:],\n","                    mode=self.interpolate_mode,\n","                    align_corners=self.align_corners))\n","\n","        out = self.fusion_conv(torch.cat(outs, dim=1))\n","\n","        out = self.cls_seg(out)\n","        \n","        # 추가\n","        out = self.upconv(out)\n","        \n","\n","        return out\n"]},{"cell_type":"code","execution_count":58,"id":"9d2f4f24","metadata":{},"outputs":[],"source":["norm_cfg = dict(type='SyncBN', requires_grad=True)\n","\n","decode_head=dict(\n","        # type='CustomSegformerHead',\n","        in_channels=[32, 64, 160, 256],\n","        in_index=[0, 1, 2, 3],\n","        channels=256,\n","        dropout_ratio=0.1,\n","        num_classes=29,\n","        norm_cfg=norm_cfg,\n","        align_corners=False,\n","        loss_decode=dict(\n","        type='CrossEntropyLoss', use_sigmoid=True, loss_weight=1.0))"]},{"cell_type":"code","execution_count":59,"id":"baa0ad26","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["/data/ephemeral/home/level2-cv-semanticsegmentation-cv-06-lv3/mmsegmentation/mmseg/models/losses/cross_entropy_loss.py:250: UserWarning: Default ``avg_non_ignore`` is False, if you would like to ignore the certain label and average loss over non-ignore labels, which is the same with PyTorch official cross_entropy, set ``avg_non_ignore=True``.\n","  warnings.warn(\n"]}],"source":["decoder = CustomSegformerHead(**decode_head)"]},{"cell_type":"code","execution_count":60,"id":"cfe6dec6","metadata":{},"outputs":[{"data":{"text/plain":["CustomSegformerHead(\n","  input_transform=multiple_select, ignore_index=255, align_corners=False\n","  (loss_decode): CrossEntropyLoss(avg_non_ignore=False)\n","  (conv_seg): Conv2d(256, 29, kernel_size=(1, 1), stride=(1, 1))\n","  (dropout): Dropout2d(p=0.1, inplace=False)\n","  (convs): ModuleList(\n","    (0): ConvModule(\n","      (conv): Conv2d(32, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (activate): ReLU(inplace=True)\n","    )\n","    (1): ConvModule(\n","      (conv): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (activate): ReLU(inplace=True)\n","    )\n","    (2): ConvModule(\n","      (conv): Conv2d(160, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (activate): ReLU(inplace=True)\n","    )\n","    (3): ConvModule(\n","      (conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (activate): ReLU(inplace=True)\n","    )\n","  )\n","  (fusion_conv): ConvModule(\n","    (conv): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","    (bn): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (activate): ReLU(inplace=True)\n","  )\n","  (upconv): ConvTranspose2d(29, 29, kernel_size=(16, 16), stride=(8, 8), padding=(4, 4))\n",")\n","init_cfg={'type': 'Normal', 'std': 0.01, 'override': {'name': 'conv_seg'}}"]},"execution_count":60,"metadata":{},"output_type":"execute_result"}],"source":["decoder.to('cuda')"]},{"cell_type":"code","execution_count":61,"id":"1b2f3830","metadata":{},"outputs":[],"source":["output = [tensor.to('cuda') for tensor in output]  # 리스트 내부의 각 텐서를 GPU로 이동"]},{"cell_type":"code","execution_count":34,"id":"b6573092","metadata":{},"outputs":[{"data":{"text/plain":["4"]},"execution_count":34,"metadata":{},"output_type":"execute_result"}],"source":["len(output)"]},{"cell_type":"code","execution_count":null,"id":"dae8dd1b","metadata":{},"outputs":[],"source":["# import os\n","\n","# os.environ['RANK'] = '0'\n","# os.environ['WORLD_SIZE'] = '1'"]},{"cell_type":"code","execution_count":null,"id":"88cc21f3","metadata":{},"outputs":[],"source":["# import torch.distributed as dist\n","\n","# dist.init_process_group(backend='nccl')  # 'nccl'은 NVIDIA GPU 환경에서 권장됨\n","# import torch.nn as nn\n","\n","# model = nn.SyncBatchNorm.convert_sync_batchnorm(model)"]},{"cell_type":"code","execution_count":null,"id":"b3fe643a","metadata":{},"outputs":[],"source":["inputs = decoder._transform_inputs(output)"]},{"cell_type":"code","execution_count":null,"id":"411b91ce","metadata":{},"outputs":[],"source":["decoder"]},{"cell_type":"code","execution_count":35,"id":"8a7ae366","metadata":{},"outputs":[],"source":["decoder.convs[0].bn = nn.BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)"]},{"cell_type":"code","execution_count":36,"id":"ccf47f87","metadata":{},"outputs":[],"source":["decoder.convs[1].bn = nn.BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)"]},{"cell_type":"code","execution_count":37,"id":"46a79c78","metadata":{},"outputs":[],"source":["decoder.convs[2].bn = nn.BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)"]},{"cell_type":"code","execution_count":38,"id":"773591b3","metadata":{},"outputs":[],"source":["decoder.convs[3].bn = nn.BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)"]},{"cell_type":"code","execution_count":39,"id":"fcac3c06","metadata":{},"outputs":[],"source":["decoder.fusion_conv.bn = nn.BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)"]},{"cell_type":"code","execution_count":42,"id":"97d720b3","metadata":{},"outputs":[{"data":{"text/plain":["CustomSegformerHead(\n","  input_transform=multiple_select, ignore_index=255, align_corners=False\n","  (loss_decode): CrossEntropyLoss(avg_non_ignore=False)\n","  (conv_seg): Conv2d(256, 29, kernel_size=(1, 1), stride=(1, 1))\n","  (dropout): Dropout2d(p=0.1, inplace=False)\n","  (convs): ModuleList(\n","    (0): ConvModule(\n","      (conv): Conv2d(32, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (activate): ReLU(inplace=True)\n","    )\n","    (1): ConvModule(\n","      (conv): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (activate): ReLU(inplace=True)\n","    )\n","    (2): ConvModule(\n","      (conv): Conv2d(160, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (activate): ReLU(inplace=True)\n","    )\n","    (3): ConvModule(\n","      (conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (activate): ReLU(inplace=True)\n","    )\n","  )\n","  (fusion_conv): ConvModule(\n","    (conv): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","    (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (activate): ReLU(inplace=True)\n","  )\n",")\n","init_cfg={'type': 'Normal', 'std': 0.01, 'override': {'name': 'conv_seg'}}"]},"execution_count":42,"metadata":{},"output_type":"execute_result"}],"source":["decoder.to('cuda')"]},{"cell_type":"code","execution_count":43,"id":"0150f3cc","metadata":{},"outputs":[],"source":["# outs = []\n","# for idx in range(len(inputs)):\n","#     x = inputs[idx]\n","#     conv = decoder.convs[idx]\n","#     outs.append(\n","#         resize(\n","#             input=conv(x),\n","#             size=inputs[0].shape[2:],\n","#             mode=interpolate_mode,\n","#             align_corners=align_corners))\n","\n","# out = fusion_conv(torch.cat(outs, dim=1))\n","\n","# out = cls_seg(out)"]},{"cell_type":"code","execution_count":44,"id":"ea58c77c","metadata":{},"outputs":[],"source":["output = decoder(output)"]},{"cell_type":"code","execution_count":45,"id":"c1c01aee","metadata":{},"outputs":[{"data":{"text/plain":["torch.Size([1, 29, 256, 256])"]},"execution_count":45,"metadata":{},"output_type":"execute_result"}],"source":["output.shape"]},{"cell_type":"code","execution_count":53,"id":"3759ae56","metadata":{},"outputs":[],"source":["upconv = nn.ConvTranspose2d(in_channels=29, out_channels=29, kernel_size=16, stride=8, padding=4)"]},{"cell_type":"code","execution_count":54,"id":"88ab9459","metadata":{},"outputs":[{"data":{"text/plain":["ConvTranspose2d(29, 29, kernel_size=(16, 16), stride=(8, 8), padding=(4, 4))"]},"execution_count":54,"metadata":{},"output_type":"execute_result"}],"source":["upconv.to('cuda')"]},{"cell_type":"code","execution_count":55,"id":"282e5a1d","metadata":{},"outputs":[],"source":["output1 = upconv(output)"]},{"cell_type":"code","execution_count":56,"id":"143f885b","metadata":{},"outputs":[{"data":{"text/plain":["torch.Size([1, 29, 2048, 2048])"]},"execution_count":56,"metadata":{},"output_type":"execute_result"}],"source":["output1.shape"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":5}
